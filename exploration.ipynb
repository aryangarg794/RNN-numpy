{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a RNN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Returns probabilities \"\"\"\n",
    "    return np.exp(logits - np.max(logits))/np.sum(np.exp(logits - np.max(logits)), axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Rectified Linear Unit activation function \"\"\"\n",
    "    return np.fmax(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.65900114, 0.24243297, 0.09856589])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test softmax\n",
    "logits = np.array([2.0, 1.0, 0.1])\n",
    "softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_true: np.ndarray, y_hat: np.ndarray) -> float:\n",
    "    \"\"\" Cross entropy loss \"\"\"\n",
    "    return -np.sum(y_true * np.log(y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy: 0.51\n"
     ]
    }
   ],
   "source": [
    "# test loss function\n",
    "y_true = np.array([0, 1, 0, 0, 0])              # True distribution\n",
    "y_pred = np.array([0.1, 0.6, 0.1, 0.15, 0.05])  # Predicted distribution\n",
    "\n",
    "print(f\"Cross Entropy: {cross_entropy(y_true, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "\n",
    "    def __init__(self, cls) -> None:\n",
    "        functools.update_wrapper(self, cls)\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        return self.forward(*args)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        dim_hidden_units: int, \n",
    "        dim_input: int, \n",
    "        batch_size: int, \n",
    "        dim_output: int, \n",
    "        activation=np.tanh, \n",
    "        loss=cross_entropy\n",
    "    ) -> None:\n",
    "        super().__init__(self)\n",
    "        # input size is ((n_x), m) where n_x is input dimensions\n",
    "        self.Wa = np.random.randn(dim_hidden_units, dim_input+dim_hidden_units) # we assume the last shape is T_x\n",
    "        self.Wy = np.random.randn(dim_output, dim_hidden_units)\n",
    "        self.ba = np.zeros((dim_hidden_units, 1))\n",
    "        self.by = np.zeros((dim_output, 1))\n",
    "        self.activation = activation\n",
    "        self.loss = loss\n",
    "        self.caches = []\n",
    "\n",
    "    def forward(self, x: np.ndarray, hidden_state_prev: np.ndarray) -> tuple[np.ndarray, np.ndarray]: \n",
    "        \n",
    "        stack = np.vstack((x, hidden_state_prev))\n",
    "        z = self.Wa @ stack + self.ba\n",
    "        hidden_state = self.activation(z)\n",
    "        logits = self.Wy @ hidden_state + self.by\n",
    "        y_hat = softmax(logits)\n",
    "\n",
    "        cache = {}\n",
    "        cache['x'] = stack\n",
    "        cache['z'] = z\n",
    "        cache['hidden_state'] = hidden_state\n",
    "        cache['y_hat'] = y_hat \n",
    "\n",
    "        self.caches.append(cache)\n",
    "\n",
    "        return hidden_state, y_hat\n",
    "\n",
    "    def reset_sequence() -> None:\n",
    "        self.outputs = []\n",
    "        self.hidden_states = []\n",
    "\n",
    "    def compute_loss(self, y_true: np.ndarray | list[float | int]) -> float:\n",
    "        y_hats = np.stack(tuple(self.outputs), axis=-1)\n",
    "        return np.sum(self.loss(y_true, y_hats)) / len(self.outputs)\n",
    "\n",
    "    def bptt(self, y_pred, y_true, z) -> None:\n",
    "        T = len(self.outputs)\n",
    "        \n",
    "        # actually this is dL_dlogits since we integrate the derivative of the softmax inside the CEloss\n",
    "        dL_dyhat = 1/T * (y_pred - y_true[:, :, -1])\n",
    "        dL_dWy = dL_dyhat @ self.hidden_states[-1].T\n",
    "        dL_dby = np.sum(dL_dyhat, axis=-1, keepdims=True)\n",
    "        dL_ht = self.Wy.T @ dL_dyhat\n",
    "        dL_dtanh = dL_ht @ (1 - self.activation(z)**2)\n",
    "\n",
    "        print(f\"dL_dyhat: {dL_dyhat}, y_hat shape: {y_pred.shape}, dL_dyhat shape: {dL_dyhat.shape}\")\n",
    "        print(f\"dL_dWy: {dL_dWy}, Wy shape: {self.Wy.shape}, dL_dWy shape: {dL_dWy.shape}\")\n",
    "        print(f\"dL_dby: {dL_dby}, by shape: {self.by.shape}, dL_dby shape: {dL_dby.shape}\")\n",
    "        print(f\"dL_ht: {dL_ht}, ht shape: {self.hidden_states[-1].shape}, dL_ht shape: {dL_ht.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at end of sequence: 3.145069063049545\n",
      "dL_dyhat: [[ 0.09814336  0.12502991]\n",
      " [ 0.06892655 -0.24618589]\n",
      " [ 0.16626342 -0.21217735]], y_hat shape: (3, 2), dL_dyhat shape: (3, 2)\n",
      "dL_dWy: [[ 0.17917978 -0.19386396 -0.18667072  0.04367452 -0.2103113 ]\n",
      " [-0.09277223  0.12225986  0.18015811 -0.12507178  0.17825159]\n",
      " [ 0.02603107 -0.00038286  0.07888883 -0.12372106  0.05749588]], Wy shape: (3, 5), dL_dWy shape: (3, 5)\n",
      "dL_dby: [[ 0.22317326]\n",
      " [-0.17725934]\n",
      " [-0.04591392]], by shape: (3, 1), dL_dby shape: (3, 1)\n",
      "dL_ht: [[ 0.10652788 -0.3673835 ]\n",
      " [-0.21128221  0.21527115]\n",
      " [ 0.00568092  0.3644284 ]\n",
      " [ 0.13096158 -0.17791011]\n",
      " [ 0.06023004 -0.68123886]], ht shape: (5, 2), dL_ht shape: (5, 2)\n"
     ]
    }
   ],
   "source": [
    "# test rnn forward\n",
    "np.random.seed(42)\n",
    "\n",
    "batch_size = 2\n",
    "seq_length = 3\n",
    "input_size = 4\n",
    "hidden_size = 5\n",
    "output_size = 3\n",
    "cell = RNNCell(hidden_size, input_size, batch_size, output_size)\n",
    "\n",
    "input_sequence = np.random.rand(input_size, batch_size, seq_length)\n",
    "hidden_state = np.zeros((hidden_size, batch_size))\n",
    "\n",
    "for t in range(seq_length):\n",
    "    input_t = input_sequence[:, :, t] \n",
    "    hidden_state, y_hat = cell(input_t, hidden_state)\n",
    "    # print(f\"Time step {t + 1}: Hidden state =\\n{hidden_state}, \\nY_hat = \\n{y_hat} \")\n",
    "\n",
    "y_true = np.array([[[1, 0, 0], [0, 1, 0]], [[1, 0, 0], [0, 0, 1]], [[0, 1, 0], [0, 0, 1]]])\n",
    "print(f\"Loss at end of sequence: {cell.compute_loss(y_true)}\")\n",
    "cell.bptt(y_hat, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "\n",
    "class OneHotEncoder(Module):\n",
    "\n",
    "    def __init__(self, num_classes: int) -> None:\n",
    "        super().__init__(self)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, indices: int | np.ndarray | list[int]) -> np.ndarray:\n",
    "        if isinstance(indices, Iterable):\n",
    "            if len(indices) > self.num_classes:\n",
    "                raise ValueError(\"Cannot have more 1s than number of classes\")\n",
    "        encoding = np.zeros((self.num_classes,))\n",
    "        encoding[indices] = 1 \n",
    "        return encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 1., 0.]), array([0., 1., 1., 0., 0.]))"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test onehot encoder class\n",
    "\n",
    "encoder = OneHotEncoder(5)\n",
    "encoder(3), encoder([1, 2])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
