{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a RNN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Returns probabilities \"\"\"\n",
    "    return np.exp(logits - np.max(logits))/np.sum(np.exp(logits - np.max(logits)), axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Rectified Linear Unit activation function \"\"\"\n",
    "    return np.fmax(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.65900114, 0.24243297, 0.09856589])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test softmax\n",
    "logits = np.array([2.0, 1.0, 0.1])\n",
    "softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_true: np.ndarray, y_hat: np.ndarray) -> float:\n",
    "    \"\"\" Cross entropy loss \"\"\"\n",
    "    return -np.sum(y_true * np.log(y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy: 0.51\n"
     ]
    }
   ],
   "source": [
    "# test loss function\n",
    "y_true = np.array([0, 1, 0, 0, 0])              # True distribution\n",
    "y_pred = np.array([0.1, 0.6, 0.1, 0.15, 0.05])  # Predicted distribution\n",
    "\n",
    "print(f\"Cross Entropy: {cross_entropy(y_true, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "\n",
    "    def __init__(self, cls) -> None:\n",
    "        functools.update_wrapper(self, cls)\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        return self.forward(*args)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        dim_hidden_units: int, \n",
    "        dim_input: int, \n",
    "        batch_size: int, \n",
    "        dim_output: int, \n",
    "        activation=np.tanh, \n",
    "        loss=cross_entropy,\n",
    "        bool_pred = True\n",
    "    ) -> None:\n",
    "        super().__init__(self)\n",
    "        # input size is ((n_x), m) where n_x is input dimensions\n",
    "        self.Wxh = np.random.randn(dim_hidden_units, dim_input) * 0.01 # we assume the last shape is T_x\n",
    "        self.Whh = np.random.randn(dim_hidden_units, dim_hidden_units) * 0.01\n",
    "        self.Wy = np.random.randn(dim_output, dim_hidden_units) * 0.01 \n",
    "        self.ba = np.zeros((dim_hidden_units, 1))\n",
    "        self.by = np.zeros((dim_output, 1))\n",
    "        self.activation = activation\n",
    "        self.loss = loss\n",
    "        self.caches = []\n",
    "        self.bool_pred = bool_pred\n",
    "\n",
    "    def forward(self, x: np.ndarray, hidden_state_prev: np.ndarray) -> tuple[np.ndarray, np.ndarray]: \n",
    "        \n",
    "        stack = np.vstack((x, hidden_state_prev)) # stack the inputs together\n",
    "        Wa = np.hstack((self.Wxh, self.Whh)) # stack the matrices together\n",
    "        z = Wa @ stack + self.ba\n",
    "        hidden_state = self.activation(z)\n",
    "        logits = self.Wy @ hidden_state + self.by\n",
    "        if self.bool_pred: y_hat = softmax(logits)\n",
    "\n",
    "        cache = {}\n",
    "        cache['x'] = stack\n",
    "        cache['z'] = z\n",
    "        cache['hidden_state'] = hidden_state\n",
    "        if self.bool_pred: cache['y_hat'] = y_hat \n",
    "\n",
    "        self.caches.append(cache)\n",
    "\n",
    "        return hidden_state, y_hat if self.bool_pred else hidden_state\n",
    "\n",
    "    def reset_sequence() -> None:\n",
    "        self.outputs = []\n",
    "        self.hidden_states = []\n",
    "\n",
    "    def compute_loss(self, y_true: np.ndarray | list[float | int]) -> float:\n",
    "        outputs = [cache['y_hat'] for cache in self.caches]\n",
    "        y_hats = np.stack(tuple(outputs), axis=-1)\n",
    "        return np.sum(self.loss(y_true, y_hats)) / len(outputs)\n",
    "    \n",
    "    def parameters(self) -> list:\n",
    "        return [self.Wxh, self.Whh, self.ba, self.Wy, self.by]\n",
    "\n",
    "    def bptt(self, y_pred, y_true) -> None:\n",
    "        T = len(self.caches)\n",
    "        \n",
    "        dl_dhts = []\n",
    "        \n",
    "        dL_dWy = np.zeros_like(self.Wy)\n",
    "        dL_dby = np.zeros_like(self.by)\n",
    "        \n",
    "        for t, cache in enumerate(self.caches):\n",
    "            # each cache represents a time step\n",
    "            curr_hidden_state = cache['hidden_state']\n",
    "            z = cache['z']\n",
    "            \n",
    "            dL_dyhat = (y_pred - y_true[:, :, t])\n",
    "            dL_dWy += dL_dyhat @ curr_hidden_state.T\n",
    "            dL_dby += np.sum(dL_dyhat, axis=-1, keepdims=True)\n",
    "            dL_ht = (self.Whh.T)**(T-t+1) @ self.Wy.T @ dL_dyhat\n",
    "            dL_dtanh = dL_ht @ (1 - self.activation(z)**2).T\n",
    "\n",
    "        print(f\"dL_dyhat: {dL_dyhat}, y_hat shape: {y_pred.shape}, dL_dyhat shape: {dL_dyhat.shape}\")\n",
    "        print(f\"dL_dWy: {dL_dWy}, Wy shape: {self.Wy.shape}, dL_dWy shape: {dL_dWy.shape}\")\n",
    "        print(f\"dL_dby: {dL_dby}, by shape: {self.by.shape}, dL_dby shape: {dL_dby.shape}\")\n",
    "        print(f\"dL_ht: {dL_ht}, ht shape: {curr_hidden_state.shape}, dL_ht shape: {dL_ht.shape}\")\n",
    "        print(f\"dL_dtanh: {dL_dtanh}, z shape: {curr_hidden_state.shape}, dL_dtanh shape: {dL_dtanh.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step 1: Hidden state =\n",
      "(5, 2), \n",
      "Y_hat = \n",
      "(3, 2) \n",
      "Time step 2: Hidden state =\n",
      "(5, 2), \n",
      "Y_hat = \n",
      "(3, 2) \n",
      "Time step 3: Hidden state =\n",
      "(5, 2), \n",
      "Y_hat = \n",
      "(3, 2) \n",
      "====================================================================================================\n",
      "Loss at end of sequence: 2.1971992325361147\n",
      "dL_dyhat: [[ 0.33336584  0.33336301]\n",
      " [ 0.33331019 -0.66669399]\n",
      " [ 0.33332397 -0.66666902]], y_hat shape: (3, 2), dL_dyhat shape: (3, 2)\n",
      "dL_dWy: [[-0.00139308  0.00110765 -0.0009676   0.00908511  0.00219662]\n",
      " [ 0.00821907  0.00413507 -0.00278707  0.0033507  -0.00675695]\n",
      " [ 0.00102602  0.00217657 -0.00064715 -0.0056532  -0.00240306]], Wy shape: (3, 5), dL_dWy shape: (3, 5)\n",
      "dL_dby: [[ 1.86556336e-04]\n",
      " [-1.51406478e-04]\n",
      " [-3.51498581e-05]], by shape: (3, 1), dL_dby shape: (3, 1)\n",
      "dL_ht: [[ 0.00178467 -0.0107689 ]\n",
      " [-0.00561644  0.00662652]\n",
      " [ 0.00023716  0.01009848]\n",
      " [ 0.00428847 -0.00514094]\n",
      " [ 0.00081078 -0.01925459]], ht shape: (5, 2), dL_ht shape: (5, 2)\n",
      "dL_dtanh: [[-0.00898381 -0.00898439 -0.00898424 -0.00898284 -0.00898365]\n",
      " [ 0.00101028  0.0010106   0.00101017  0.00100989  0.00101033]\n",
      " [ 0.01033504  0.0103356   0.0103356   0.01033405  0.01033483]\n",
      " [-0.00085263 -0.00085288 -0.00085255 -0.00085232 -0.00085266]\n",
      " [-0.01844282 -0.01844387 -0.01844378 -0.01844098 -0.01844245]], z shape: (5, 2), dL_dtanh shape: (5, 5)\n"
     ]
    }
   ],
   "source": [
    "# test rnn forward and backward\n",
    "np.random.seed(42)\n",
    "\n",
    "batch_size = 2\n",
    "seq_length = 3\n",
    "input_size = 4\n",
    "hidden_size = 5\n",
    "output_size = 3\n",
    "cell = RNNCell(hidden_size, input_size, batch_size, output_size)\n",
    "\n",
    "input_sequence = np.random.rand(input_size, batch_size, seq_length)\n",
    "hidden_state = np.zeros((hidden_size, batch_size))\n",
    "\n",
    "for t in range(seq_length):\n",
    "    input_t = input_sequence[:, :, t] \n",
    "    hidden_state, y_hat = cell(input_t, hidden_state)\n",
    "    print(f\"Time step {t + 1}: Hidden state =\\n{hidden_state.shape}, \\nY_hat = \\n{y_hat.shape} \")\n",
    "\n",
    "print(f'{'=' * 100}')\n",
    "\n",
    "y_true = np.array([[[1, 0, 0], [0, 1, 0]], [[1, 0, 0], [0, 0, 1]], [[0, 1, 0], [0, 0, 1]]])\n",
    "print(f\"Loss at end of sequence: {cell.compute_loss(y_true)}\")\n",
    "cell.bptt(y_hat, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test parameters are returned\n",
    "np.random.seed(42)\n",
    "\n",
    "batch_size = 2\n",
    "seq_length = 3\n",
    "input_size = 4\n",
    "hidden_size = 5\n",
    "output_size = 3\n",
    "cell2 = RNNCell(hidden_size, input_size, batch_size, output_size)\n",
    "params = cell2.parameters()\n",
    "\n",
    "assert np.allclose(cell2.Wxh, params[0])\n",
    "assert np.allclose(cell2.Whh, params[1])\n",
    "assert np.allclose(cell2.ba, params[2])\n",
    "assert np.allclose(cell2.Wy, params[3])\n",
    "assert np.allclose(cell2.by, params[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "\n",
    "class OneHotEncoder(Module):\n",
    "\n",
    "    def __init__(self, num_classes: int) -> None:\n",
    "        super().__init__(self)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, indices: int | np.ndarray | list[int]) -> np.ndarray:\n",
    "        if isinstance(indices, Iterable):\n",
    "            if len(indices) > self.num_classes:\n",
    "                raise ValueError(\"Cannot have more 1s than number of classes\")\n",
    "        encoding = np.zeros((self.num_classes,))\n",
    "        encoding[indices] = 1 \n",
    "        return encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 1., 0.]), array([0., 1., 1., 0., 0.]))"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test onehot encoder class\n",
    "\n",
    "encoder = OneHotEncoder(5)\n",
    "encoder(3), encoder([1, 2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
