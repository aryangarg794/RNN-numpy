{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a RNN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Returns probabilities \"\"\"\n",
    "    return np.exp(logits - np.max(logits))/np.sum(np.exp(logits - np.max(logits)), axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Rectified Linear Unit activation function \"\"\"\n",
    "    return np.fmax(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.65900114, 0.24243297, 0.09856589])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test softmax\n",
    "logits = np.array([2.0, 1.0, 0.1])\n",
    "softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_true: np.ndarray, y_hat: np.ndarray) -> float:\n",
    "    \"\"\" Cross entropy loss \"\"\"\n",
    "    return -np.sum(y_true * np.log(y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy: 0.51\n"
     ]
    }
   ],
   "source": [
    "# test loss function\n",
    "y_true = np.array([0, 1, 0, 0, 0])              # True distribution\n",
    "y_pred = np.array([0.1, 0.6, 0.1, 0.15, 0.05])  # Predicted distribution\n",
    "\n",
    "print(f\"Cross Entropy: {cross_entropy(y_true, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "\n",
    "    def __init__(self, cls) -> None:\n",
    "        functools.update_wrapper(self, cls)\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        return self.forward(*args)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        dim_hidden_units: int, \n",
    "        dim_input: int, \n",
    "        batch_size: int, \n",
    "        dim_output: int, \n",
    "        activation=np.tanh, \n",
    "        loss=cross_entropy,\n",
    "        bool_pred = True\n",
    "    ) -> None:\n",
    "        super().__init__(self)\n",
    "        # input size is ((n_x), m) where n_x is input dimensions\n",
    "        self.Wxh = np.random.randn(dim_hidden_units, dim_input) # we assume the last shape is T_x\n",
    "        self.Whh = np.random.randn(dim_hidden_units, dim_hidden_units)\n",
    "        self.Wy = np.random.randn(dim_output, dim_hidden_units) \n",
    "        self.ba = np.zeros((dim_hidden_units, 1))\n",
    "        self.by = np.zeros((dim_output, 1))\n",
    "        self.activation = activation\n",
    "        self.loss = loss\n",
    "        self.caches = []\n",
    "        self.bool_pred = bool_pred\n",
    "\n",
    "    def forward(self, x: np.ndarray, hidden_state_prev: np.ndarray) -> tuple[np.ndarray, np.ndarray]: \n",
    "        \n",
    "        stack = np.vstack((x, hidden_state_prev)) # stack the inputs together\n",
    "        Wa = np.hstack((self.Wxh, self.Whh)) # stack the matrices together\n",
    "        z = Wa @ stack + self.ba\n",
    "        hidden_state = self.activation(z)\n",
    "        logits = self.Wy @ hidden_state + self.by\n",
    "        if self.bool_pred: y_hat = softmax(logits)\n",
    "\n",
    "        cache = {}\n",
    "        cache['x'] = x\n",
    "        cache['hidden_state_prev'] = hidden_state_prev\n",
    "        cache['z'] = z\n",
    "        cache['hidden_state'] = hidden_state\n",
    "        if self.bool_pred: cache['y_hat'] = y_hat \n",
    "\n",
    "        self.caches.append(cache)\n",
    "\n",
    "        return hidden_state, y_hat if self.bool_pred else hidden_state\n",
    "\n",
    "    def reset_sequence(self) -> None:\n",
    "        self.caches = []\n",
    "\n",
    "    def compute_loss(self, y_true: np.ndarray | list[float | int]) -> float:\n",
    "        outputs = [cache['y_hat'] for cache in self.caches]\n",
    "        y_hats = np.stack(tuple(outputs), axis=-1)\n",
    "        return np.sum(self.loss(y_true, y_hats)) / len(outputs)\n",
    "    \n",
    "    def parameters(self) -> list:\n",
    "        return [self.Wxh, self.Whh, self.ba, self.Wy, self.by]\n",
    "\n",
    "    def bptt(self, y_true: np.ndarray, verbose=False) -> list:\n",
    "        T = len(self.caches)\n",
    "\n",
    "        dL_dWy = np.zeros_like(self.Wy)\n",
    "        dL_dby = np.zeros_like(self.by)\n",
    "        dL_dWxh = np.zeros_like(self.Wxh)\n",
    "        dL_dWhh = np.zeros_like(self.Whh)\n",
    "        dL_dba = np.zeros_like(self.ba)\n",
    "        \n",
    "        dL_dht_next = np.zeros_like(self.caches[0]['hidden_state'])\n",
    "        \n",
    "        for t, cache in enumerate(self.caches[::-1]):\n",
    "            # each cache represents a time step\n",
    "            curr_hidden_state = cache['hidden_state']\n",
    "            z = cache['z']\n",
    "            y_pred = cache['y_hat']\n",
    "            \n",
    "            dL_dyhat = y_pred - y_true[:, :, t]\n",
    "            dL_dht = self.Wy.T @ dL_dyhat + dL_dht_next\n",
    "            dL_dtanh = dL_dht * (1 - curr_hidden_state ** 2)\n",
    "                        \n",
    "            dL_dWy += dL_dyhat @ curr_hidden_state.T\n",
    "            dL_dby += np.sum(dL_dyhat, axis=-1, keepdims=True)\n",
    "            \n",
    "            x = cache['x']\n",
    "            hidden_state_prev = cache['hidden_state_prev']\n",
    "            dL_dWxh += dL_dtanh @ x.T  \n",
    "            dL_dWhh += dL_dtanh @ hidden_state_prev.T\n",
    "            dL_dba += np.sum(dL_dtanh, axis=-1, keepdims=True)\n",
    "            \n",
    "            dL_dht_next = self.Whh.T @ dL_dtanh\n",
    "            \n",
    "        return [dL_dWxh, dL_dWhh, dL_dba, dL_dWy, dL_dby]\n",
    "    \n",
    "        if verbose:\n",
    "            print(f\"dL_dyhat: {dL_dyhat}, y_hat shape: {y_pred.shape}, dL_dyhat shape: {dL_dyhat.shape}\")\n",
    "            print(f\"dL_dWy: {dL_dWy}, Wy shape: {self.Wy.shape}, dL_dWy shape: {dL_dWy.shape}\")\n",
    "            print(f\"dL_dby: {dL_dby}, by shape: {self.by.shape}, dL_dby shape: {dL_dby.shape}\")\n",
    "            print(f\"dL_ht: {dL_ht}, ht shape: {curr_hidden_state.shape}, dL_ht shape: {dL_ht.shape}\")\n",
    "            print(f\"dL_dtanh: {dL_dtanh}, z shape: {curr_hidden_state.shape}, dL_dtanh shape: {dL_dtanh.shape}\")\n",
    "        \n",
    "    def optimizer_step(self, learning_rate: float, grads: list) -> None:\n",
    "        params = self.parameters()\n",
    "        for param, grad in zip(params, grads):\n",
    "            param -= learning_rate * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step 1: Hidden state =\n",
      "(5, 2), \n",
      "Y_hat = \n",
      "(3, 2) \n",
      "Time step 2: Hidden state =\n",
      "(5, 2), \n",
      "Y_hat = \n",
      "(3, 2) \n",
      "Time step 3: Hidden state =\n",
      "(5, 2), \n",
      "Y_hat = \n",
      "(3, 2) \n",
      "====================================================================================================\n",
      "Loss at end of sequence: 2.1971992325361147\n"
     ]
    }
   ],
   "source": [
    "# test rnn forward and backward\n",
    "np.random.seed(42)\n",
    "\n",
    "batch_size = 2\n",
    "seq_length = 3\n",
    "input_size = 4\n",
    "hidden_size = 5\n",
    "output_size = 3\n",
    "cell = RNNCell(hidden_size, input_size, batch_size, output_size)\n",
    "\n",
    "input_sequence = np.random.rand(input_size, batch_size, seq_length)\n",
    "hidden_state = np.zeros((hidden_size, batch_size))\n",
    "\n",
    "for t in range(seq_length):\n",
    "    input_t = input_sequence[:, :, t] \n",
    "    hidden_state, y_hat = cell(input_t, hidden_state)\n",
    "    print(f\"Time step {t + 1}: Hidden state =\\n{hidden_state.shape}, \\nY_hat = \\n{y_hat.shape} \")\n",
    "\n",
    "print(f'{'=' * 100}')\n",
    "\n",
    "y_true = np.array([[[1, 0, 0], [0, 1, 0]], [[1, 0, 0], [0, 0, 1]], [[0, 1, 0], [0, 0, 1]]])\n",
    "print(f\"Loss at end of sequence: {cell.compute_loss(y_true)}\")\n",
    "grads = cell.bptt(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test parameters are returned\n",
    "np.random.seed(42)\n",
    "\n",
    "batch_size = 2\n",
    "seq_length = 3\n",
    "input_size = 4\n",
    "hidden_size = 5\n",
    "output_size = 3\n",
    "cell2 = RNNCell(hidden_size, input_size, batch_size, output_size)\n",
    "params = cell2.parameters()\n",
    "\n",
    "assert np.allclose(cell2.Wxh, params[0])\n",
    "assert np.allclose(cell2.Whh, params[1])\n",
    "assert np.allclose(cell2.ba, params[2])\n",
    "assert np.allclose(cell2.Wy, params[3])\n",
    "assert np.allclose(cell2.by, params[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "\n",
    "class OneHotEncoder(Module):\n",
    "\n",
    "    def __init__(self, num_classes: int) -> None:\n",
    "        super().__init__(self)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, indices: int | np.ndarray | list[int]) -> np.ndarray:\n",
    "        if isinstance(indices, Iterable):\n",
    "            if len(indices) > self.num_classes:\n",
    "                raise ValueError(\"Cannot have more 1s than number of classes\")\n",
    "        encoding = np.zeros((self.num_classes,))\n",
    "        encoding[indices] = 1 \n",
    "        return encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 1., 0.]), array([0., 1., 1., 0., 0.]))"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test onehot encoder class\n",
    "\n",
    "encoder = OneHotEncoder(5)\n",
    "encoder(3), encoder([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing gradients between Custom RNN and Torch RNN:\n",
      "Gradient for Wxh:\n",
      "Allclose:\n",
      "False\n",
      "Gradient for Whh:\n",
      "Allclose:\n",
      "False\n",
      "Gradient for ba:\n",
      "Allclose:\n",
      "False\n",
      "Gradient for Wy:\n",
      "Allclose:\n",
      "False\n",
      "Gradient for by:\n",
      "Allclose:\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TorchRNNCell(nn.Module):\n",
    "    def __init__(self, dim_hidden_units, dim_input, dim_output):\n",
    "        super(TorchRNNCell, self).__init__()\n",
    "        self.Wxh = nn.Parameter(torch.randn(dim_hidden_units, dim_input))\n",
    "        self.Whh = nn.Parameter(torch.randn(dim_hidden_units, dim_hidden_units))\n",
    "        self.Wy = nn.Parameter(torch.randn(dim_output, dim_hidden_units))\n",
    "        self.ba = nn.Parameter(torch.zeros(dim_hidden_units, 1))\n",
    "        self.by = nn.Parameter(torch.zeros(dim_output, 1))\n",
    "\n",
    "    def forward(self, x, hidden_state_prev):\n",
    "        stack = torch.cat((x, hidden_state_prev), dim=0)\n",
    "        Wa = torch.cat((self.Wxh, self.Whh), dim=1)\n",
    "        z = Wa @ stack + self.ba\n",
    "        hidden_state = torch.tanh(z)\n",
    "        logits = self.Wy @ hidden_state + self.by\n",
    "        y_hat = F.softmax(logits, dim=0)\n",
    "        return hidden_state, y_hat\n",
    "\n",
    "def compute_loss_torch(y_pred, y_true):\n",
    "    batch_size, seq_len = y_true.size(1), y_true.size(2)\n",
    "    y_true_reshaped = y_true.permute(1, 2, 0).contiguous().view(-1, y_true.size(0))\n",
    "    y_pred_repeated = y_pred.unsqueeze(1).expand(-1, seq_len, -1).contiguous().view(-1, y_pred.size(0))\n",
    "    \n",
    "    if y_true_reshaped.dtype == torch.long:\n",
    "        y_true_reshaped = y_true_reshaped.float()\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    return loss_fn(y_pred_repeated, y_true_reshaped)\n",
    "\n",
    "\n",
    "def compare_gradients(custom_rnn, torch_rnn, x_np, hidden_np, y_true_np, learning_rate=0.01):\n",
    "    custom_rnn.reset_sequence()\n",
    "    hidden_custom, y_hat_custom = custom_rnn.forward(x_np, hidden_np)\n",
    "    custom_loss = custom_rnn.compute_loss(y_true_np)\n",
    "    grads_custom = custom_rnn.bptt(y_true_np)\n",
    "    \n",
    "    x_torch = torch.tensor(x_np, dtype=torch.float32, requires_grad=True)\n",
    "    hidden_torch = torch.tensor(hidden_np, dtype=torch.float32, requires_grad=True)\n",
    "    y_true_torch = torch.tensor(y_true_np, dtype=torch.long)\n",
    "    hidden_torch_out, y_hat_torch = torch_rnn(x_torch, hidden_torch)\n",
    "    loss_torch = compute_loss_torch(y_hat_torch, y_true_torch)\n",
    "    loss_torch.backward()\n",
    "\n",
    "    grads_torch = {\n",
    "        'Wxh': torch_rnn.Wxh.grad.detach().numpy(),\n",
    "        'Whh': torch_rnn.Whh.grad.detach().numpy(),\n",
    "        'ba': torch_rnn.ba.grad.detach().numpy(),\n",
    "        'Wy': torch_rnn.Wy.grad.detach().numpy(),\n",
    "        'by': torch_rnn.by.grad.detach().numpy()\n",
    "    }\n",
    "\n",
    "    grads_custom_dict = {\n",
    "        'Wxh': grads_custom[0],\n",
    "        'Whh': grads_custom[1],\n",
    "        'ba': grads_custom[2],\n",
    "        'Wy': grads_custom[3],\n",
    "        'by': grads_custom[4]\n",
    "    }\n",
    "\n",
    "    print(\"Comparing gradients between Custom RNN and Torch RNN:\")\n",
    "    for name, grad_torch in grads_torch.items():\n",
    "        grad_custom = grads_custom_dict[name]\n",
    "        print(f\"Gradient for {name}:\")\n",
    "        # print(f\"Custom RNN grad:\\n{grad_custom}\")\n",
    "        # print(f\"Torch RNN grad:\\n{grad_torch}\")\n",
    "        # print(f\"Difference:\\n{np.abs(grad_custom - grad_torch)}\\n\")\n",
    "        print(f'Allclose:\\n{np.allclose(grad_custom, grad_torch)}')\n",
    "\n",
    "dim_hidden_units = 5\n",
    "dim_input = 4\n",
    "dim_output = 3\n",
    "batch_size = 2\n",
    "seq_length = 3\n",
    "\n",
    "custom_rnn = RNNCell(dim_hidden_units, dim_input, batch_size, dim_output)\n",
    "torch_rnn = TorchRNNCell(dim_hidden_units, dim_input, dim_output)\n",
    "\n",
    "x_np = np.random.randn(dim_input, batch_size)\n",
    "hidden_np = np.random.randn(dim_hidden_units, batch_size)\n",
    "y_true_np = np.array([[[1, 0, 0], [0, 1, 0]], [[1, 0, 0], [0, 0, 1]], [[0, 1, 0], [0, 0, 1]]])\n",
    "\n",
    "compare_gradients(custom_rnn, torch_rnn, x_np, hidden_np, y_true_np)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
